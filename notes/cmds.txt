## Commands for pytorch container

sudo apt update && sudo apt -y upgrade
sudo apt install -y neovim htop atop bmon tree python3.10-venv zsh

python3 -m venv env --system-site-packages
source env/bin/activate
pip install --upgrade pip


git config --global user.email "adam.lesnikowski@gmail.com"
git config --global user.name "Adam Lesnikowski"
git config --global credential.helper cache
git config --global credential.helper 'cache --timeout=36000'

git clone https://github.com/lesnikow/direct-preference-optimization.git
cd direct-preference-optimization
git checkout ab

pip install -r requirements-pytorch-container.txt

wandb login 3df7ad506a96b198d251a4df07f7c9b5bd4745e3


## Commands for cuda container

sudo apt update && sudo apt -y upgrade
sudo apt install -y neovim htop atop bmon tree python3.10-venv zsh

python3 -m venv env
source env/bin/activate
pip install --upgrade pip

git clone https://github.com/lesnikow/direct-preference-optimization.git
cd direct-preference-optimization
git checkout main

pip install -r requirements.txt
pip install --upgrade datasets wandb
wandb login 3df7ad506a96b198d251a4df07f7c9b5bd4745e3



## Basic experiment setup, replicated dpo codebase


[ ] Dataset DCPO, loss DPO

[ ] Dataset DCPO, loss SFT

[ ] Dataset DPO,  loss DPO

[ ] Dataset DPO,  loss SFT





## DPO

### 4 x H100 80 GB, ulimit, more freq evals, dpo dataset, dpo loss
ulimit -n 64000; python -u train.py model=pythia28 datasets=[dpo] loss=dpo loss.beta=0.1
exp_name=dataset_dpo_loss_dpo gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 eval_every=10000 model.archive=.cache/root/.../LATEST/policy.pt


### 4 x H100 80 GB, ulimit, more freq evals
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=dpo loss.beta=0.1 exp_name=anthropic_dpo_pythia28_dpo gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 eval_every=10000 model.archive=.cache/root/.../LATEST/policy.pt

### OG command DPO
python -u train.py model=pythia28 datasets=[hh] loss=dpo loss.beta=0.1 exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 model.archive=/path/to/archive/from/sft/LATEST/policy.pt


## SFT

### 4 x H100 80 GB, ulimit, more freq evals, dpo datset, sft loss
ulimit -n 64000; python -u train.py model=pythia28 datasets=[dpo] loss=sft
exp_name=dataset_dpo_loss_sft gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 eval_every=10000


### 4 x H100 80 GB, more freq evals
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16 eval_every=10000

### 4 x H100 80 GB 
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16

### 2 x H100 80 GB 
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=4 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16

### 1 x H100 80 GB, stable at close to 65 GB memory used
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=8 batch_size=16 eval_batch_size=8 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16


### OG command SFT
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16


