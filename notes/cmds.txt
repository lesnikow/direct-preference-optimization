sudo apt update
sudo apt -y upgrade
sudo apt install -y neovim htop atop bmon tree python3.10-venv zsh

python3 -m venv env
source env/bin/activate
pip install --upgrade pip

git clone https://github.com/lesnikow/direct-preference-optimization.git
cd direct-preference-optimization
git checkout ct

pip install -r requirements.txt
# pip install --index-url https://pypi.org/simple/ -r requirements.txt
# pip install --index-url https://repo.anaconda.com/pkgs/ -r requirements.txt

pip install --upgrade datasets wandb
wandb login 3df7ad506a96b198d251a4df07f7c9b5bd4745e3


# 1 x H100 80 GB stable at close to 65 GB memory used
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=8 batch_size=16 eval_batch_size=8 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16


# 1 x H100 80 GB OOM while training
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16


# OG command
ulimit -n 64000; python -u train.py model=pythia28 datasets=[hh] loss=sft exp_name=anthropic_dpo_pythia28 gradient_accumulation_steps=2 batch_size=64 eval_batch_size=32 trainer=FSDPTrainer sample_during_eval=false model.fsdp_policy_mp=bfloat16
